{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dde4fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9b88841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bef9e9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a20b4959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "20dbb0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "384eace1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd4f7083",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53649f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3386, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "76c48ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# dcounts = 0\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n\n",
    "dprobs = (1.0/ probs) * dlogprobs\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdims=True)\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "dnorm_logits= counts * dcounts\n",
    "dlogits = dnorm_logits.clone()\n",
    "dlogit_maxes = (-dnorm_logits.sum(1, keepdims=True))\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "dh = dlogits @ W2.T\n",
    "dW2= h.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "dhpreact = (1.0 - h**2) * dh\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "dbnraw = bngain * dhpreact\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "dbndiff += (2*bndiff) * dbndiff2\n",
    "dhprebn = dbndiff.clone()\n",
    "dbnmeani = (-dbndiff).sum(0)\n",
    "dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "demb = dembcat.view(emb.shape)\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "  for j in range(Xb.shape[1]):\n",
    "    ix = Xb[k,j]\n",
    "    dC[ix] += demb[k,j]\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e3bcd0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.338604211807251 diff: -2.384185791015625e-07\n"
     ]
    }
   ],
   "source": [
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "46a22b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "dlogits = F.softmax(logits, 1)\n",
    "dlogits[range(n), Yb] -= 1\n",
    "dlogits /= n\n",
    "\n",
    "cmp('logits', dlogits, logits) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "10639a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, Yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "464c40bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0736, 0.0871, 0.0187, 0.0489, 0.0189, 0.0855, 0.0252, 0.0385, 0.0177,\n",
       "        0.0303, 0.0323, 0.0394, 0.0376, 0.0283, 0.0352, 0.0135, 0.0097, 0.0189,\n",
       "        0.0159, 0.0524, 0.0531, 0.0211, 0.0265, 0.0653, 0.0580, 0.0247, 0.0237],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d6b2172b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0736,  0.0871,  0.0187,  0.0489,  0.0189,  0.0855,  0.0252,  0.0385,\n",
       "        -0.9823,  0.0303,  0.0323,  0.0394,  0.0376,  0.0283,  0.0352,  0.0135,\n",
       "         0.0097,  0.0189,  0.0159,  0.0524,  0.0531,  0.0211,  0.0265,  0.0653,\n",
       "         0.0580,  0.0247,  0.0237], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0] * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0eaf11f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4.6566e-09, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1cee4d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2438cd04640>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(dlogits.detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd326437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
